{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 許恩嘉\n",
    "\n",
    "Student ID: 113065530\n",
    "\n",
    "GitHub ID: 92287531\n",
    "\n",
    "Kaggle name: andyhs\n",
    "\n",
    "Kaggle private scoreboard snapshot:![rank_snapshot](rank.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is Kaggle enviroment first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To avoid messing up the layout, we will import all the required libraries in a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insatll package\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import nltk\n",
    "import spacy\n",
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM = r\"/kaggle/input/dm-2024-isa-5810-lab-2-homework/tweets_DM.json\"\n",
    "data_identitfication = r\"/kaggle/input/dm-2024-isa-5810-lab-2-homework/data_identification.csv\"\n",
    "emotion = r\"/kaggle/input/dm-2024-isa-5810-lab-2-homework/emotion.csv\"\n",
    "sample_submission = r\"/kaggle/input/dm-2024-isa-5810-lab-2-homework/sampleSubmission.csv\"\n",
    "\n",
    "sample_submission_csv = pd.read_csv(sample_submission)\n",
    "data_identitfication_csv = pd.read_csv(data_identitfication)\n",
    "emotion_csv = pd.read_csv(emotion)\n",
    "tweets_DM_json = pd.read_json(tweets_DM, lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract hashtags, tweet_id and text from _source column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_df = pd.DataFrame(tweets_DM_json)\n",
    "\n",
    "tweets_DM_new = tweets_DM_df[['_source']]\n",
    "tweets_DM_new['hashtags'] = tweets_DM_new['_source'].apply(lambda x: x['tweet']['hashtags'])\n",
    "tweets_DM_new['tweet_id'] = tweets_DM_new['_source'].apply(lambda x: x['tweet']['tweet_id'])\n",
    "tweets_DM_new['text'] = tweets_DM_new['_source'].apply(lambda x: x['tweet']['text'])\n",
    "\n",
    "tweets_DM_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do some merge to concat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_new = pd.merge(tweets_DM_new, emotion_csv, how = \"left\", on = ['tweet_id'])\n",
    "tweets_DM_new = pd.merge(tweets_DM_new, data_identitfication_csv, how = \"left\", on = ['tweet_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## design some function utilized to do data preprocessing\n",
    "### clean_at: used to remove @[user]\n",
    "### to_lower: change all letter to lower\n",
    "### clean_LH: \\<LH> consists of words that have already been removed by Twitter's official system. Since their removal does not follow any specific rules, it is impossible to determine their purpose. Therefore, I chose to remove them.\n",
    "### standard_space: it is designed to transform more than one space to one space\n",
    "### remove_url: remove some url, which is not important for emotional analysis\n",
    "### handle_hashtag: do some tricks to remove hashtag in the content and add it to hashtag column\n",
    "### tokenize_and_remove_stopwords: split the word and remove some stopwords\n",
    "## and other function not mentioned above is used to speed up the data preprocessing, because the dataset is too big\n",
    "\n",
    "## In the end, I choose to use model, BERTtweet, which is specified training for twitter analysis. According to its abilities, I only use \"clean_at\", \"clean_LH\", \"remove_url\" and \"standard_space\" functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_at(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    return text\n",
    "\n",
    "def to_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def clean_LH(text):\n",
    "    text = text.replace(\"<LH>\", '')\n",
    "    return text\n",
    "\n",
    "def standard_space(text):\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    return text\n",
    "    \n",
    "def remove_url(text):\n",
    "    # 使用正則表達式匹配 www. 開頭和 .com 結尾的網址\n",
    "    # 這個正則表達式會匹配以 \"www.\" 開頭且以 \".com\" 結尾的網址\n",
    "    text = re.sub(r'http[s]?://(?:www\\.)?[\\w-]+\\.[a-z]{2,}/?', '', text)\n",
    "    return text\n",
    "\n",
    "def handle_hashtag(text, hashtags):\n",
    "    if \"#\" in text:\n",
    "        found_hashtags = re.findall(r'#\\w+', text)\n",
    "        \n",
    "        for hashtag in found_hashtags:\n",
    "            clean_hashtag = hashtag.lstrip('#')\n",
    "            \n",
    "            if clean_hashtag not in hashtags:\n",
    "                hashtags.append(clean_hashtag)\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "# 將處理函數移到最外層\n",
    "def process_single_row(args):\n",
    "    \"\"\"\n",
    "    處理單行數據的函數\n",
    "    \"\"\"\n",
    "    row, text_column, hashtags_column, pattern = args\n",
    "    text = str(row[text_column])\n",
    "    current_hashtags = row[hashtags_column]\n",
    "    \n",
    "    # 確保 current_hashtags 是列表\n",
    "    if not isinstance(current_hashtags, list):\n",
    "        current_hashtags = []\n",
    "    \n",
    "    if '#' in text:\n",
    "        # 找出所有 hashtags\n",
    "        found_tags = pattern.findall(text)\n",
    "        # 移除 # 符號並轉換為集合\n",
    "        new_tags = {tag.lstrip('#') for tag in found_tags}\n",
    "        # 現有的 hashtags 轉換為集合\n",
    "        current_tags = set(current_hashtags)\n",
    "        # 合併並轉回列表\n",
    "        return list(current_tags.union(new_tags))\n",
    "    \n",
    "    return current_hashtags\n",
    "\n",
    "def process_chunk(args):\n",
    "    \"\"\"\n",
    "    處理數據塊的函數\n",
    "    \"\"\"\n",
    "    chunk, text_column, hashtags_column, pattern = args\n",
    "    result = []\n",
    "    for _, row in chunk.iterrows():\n",
    "        result.append(process_single_row((row, text_column, hashtags_column, pattern)))\n",
    "    return result\n",
    "\n",
    "def process_hashtags_fast_v2(df, text_column='text', hashtags_column='hashtags', batch_size=10000):\n",
    "    \"\"\"\n",
    "    快速處理大量推文中的 hashtags，使用批次處理而不是多進程\n",
    "    \n",
    "    Parameters:\n",
    "        df: 輸入的 DataFrame\n",
    "        text_column: 包含文本的列名\n",
    "        hashtags_column: 包含現有 hashtags 的列名\n",
    "        batch_size: 每批處理的行數\n",
    "    \n",
    "    Returns:\n",
    "        處理後的 hashtags Series\n",
    "    \"\"\"\n",
    "    # 預編譯正則表達式\n",
    "    pattern = re.compile(r'#\\w+')\n",
    "    \n",
    "    # 初始化結果列表\n",
    "    results = []\n",
    "    \n",
    "    # 批次處理數據\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # 處理當前批次\n",
    "        batch_results = process_chunk((batch, text_column, hashtags_column, pattern))\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # 顯示進度\n",
    "        if (start_idx // batch_size) % 10 == 0:\n",
    "            print(f\"已處理 {start_idx + len(batch)}/{len(df)} 行 ({((start_idx + len(batch))/len(df)*100):.1f}%)\")\n",
    "    \n",
    "    return pd.Series(results, index=df.index)\n",
    "    \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "def preprocess_batch(texts, stop_words_set, batch_size=1000):\n",
    "    \"\"\"\n",
    "    批次處理文本的函數\n",
    "    \n",
    "    Parameters:\n",
    "        texts: 要處理的文本列表\n",
    "        stop_words_set: 停用詞集合\n",
    "        batch_size: 批次大小\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total = len(texts)\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = texts[i:min(i + batch_size, total)]\n",
    "        batch_results = []\n",
    "        \n",
    "        for text in batch:\n",
    "            if pd.isna(text):\n",
    "                batch_results.append([])\n",
    "                continue\n",
    "                \n",
    "            # 使用快速的列表解析\n",
    "            words = word_tokenize(str(text))\n",
    "            cleaned = [word for word in words if word not in stop_words_set]\n",
    "            batch_results.append(cleaned)\n",
    "            \n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # 顯示進度\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"已處理 {i + len(batch)}/{total} 行 ({((i + len(batch))/total*100):.1f}%)\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "def fast_text_cleaning(df, text_column='text', batch_size=1000, n_jobs=None):\n",
    "    \"\"\"\n",
    "    快速的文本清理函數\n",
    "    \n",
    "    Parameters:\n",
    "        df: 輸入的 DataFrame\n",
    "        text_column: 文本列的名稱\n",
    "        batch_size: 每個批次處理的行數\n",
    "        n_jobs: 並行處理的作業數，None 表示使用所有可用的 CPU 核心\n",
    "    \"\"\"\n",
    "    # 將停用詞轉換為集合以加快查詢速度\n",
    "    stop_words_set = set(stop_words)\n",
    "    \n",
    "    if n_jobs and n_jobs > 1:\n",
    "        splits = np.array_split(df[text_column], n_jobs)\n",
    "        \n",
    "        process_func = partial(preprocess_batch, stop_words_set=stop_words_set, batch_size=batch_size)\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            results = []\n",
    "            for batch_result in executor.map(process_func, splits):\n",
    "                results.extend(batch_result)\n",
    "                \n",
    "        return pd.Series(results, index=df.index)\n",
    "    else:\n",
    "        return pd.Series(\n",
    "            preprocess_batch(df[text_column].values, stop_words_set, batch_size),\n",
    "            index=df.index\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_new[\"text\"] = tweets_DM_new[\"text\"].apply(lambda x: clean_at(x))\n",
    "tweets_DM_new[\"text\"] = tweets_DM_new[\"text\"].apply(lambda x: remove_url(x))\n",
    "tweets_DM_new[\"text\"] = tweets_DM_new[\"text\"].apply(lambda x: clean_LH(x))\n",
    "tweets_DM_new[\"text\"] = tweets_DM_new[\"text\"].apply(lambda x: standard_space(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_DM_new.to_pickle(r'/kaggle/working/tweets_DM_new_for_BERTtweet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utilizing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # I detect there still exist some <LH>, so I remove it again\n",
    "    text = re.sub(r'<LH>', '', text)  # 移除 <LH> 標記\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            actual_labels.extend(labels.cpu().tolist())\n",
    "    \n",
    "    return predictions, actual_labels\n",
    "\n",
    "def main():\n",
    "    # set cuda\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # load data\n",
    "    df = pd.read_pickle('tweets_DM_new_for_BERTtweet.pkl')\n",
    "    \n",
    "    # preprocessing data again \n",
    "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "    \n",
    "    # split train and test\n",
    "    train_df = df[df['identification'] == 'train'].copy()\n",
    "    test_df = df[df['identification'] == 'test'].copy()\n",
    "    \n",
    "    # create label\n",
    "    label_dict = {label: idx for idx, label in enumerate(train_df['emotion'].unique())}\n",
    "    train_df['label'] = train_df['emotion'].map(label_dict)\n",
    "    \n",
    "    # import BERTweet\n",
    "    tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'vinai/bertweet-base',\n",
    "        num_labels=len(label_dict)\n",
    "    ).to(device)\n",
    "    \n",
    "    # create TwitterDataset\n",
    "    train_dataset = TwitterDataset(\n",
    "        texts=train_df['processed_text'].values,\n",
    "        labels=train_df['label'].values,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # create DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # set lr=0.00002 and train for three epochs\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 3\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        print(f\"Average training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # do prediction on test_df\n",
    "    test_dataset = TwitterDataset(\n",
    "        texts=test_df['processed_text'].values,\n",
    "        labels=[0] * len(test_df), \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    predictions, _ = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    # map numerical label to original label\n",
    "    reverse_label_dict = {v: k for k, v in label_dict.items()}\n",
    "    predicted_emotions = [reverse_label_dict[pred] for pred in predictions]\n",
    "    \n",
    "    test_df['predicted_emotion'] = predicted_emotions\n",
    "    \n",
    "    # output some example\n",
    "    print(\"\\n預測範例:\")\n",
    "    for text, pred in zip(test_df['text'].head(), test_df['predicted_emotion'].head()):\n",
    "        print(f\"\\n原始文本: {text}\")\n",
    "        print(f\"預測情緒: {pred}\")\n",
    "    \n",
    "    # save to csv\n",
    "    test_df.to_csv('predictions.csv', index=False)\n",
    "    \n",
    "    print(\"\\n預測結果已保存到 predictions.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because I forget putting tweet_id into the training and testing data, I used another .py to merge this two column\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "import ast\n",
    "\n",
    "# Function to safely parse the '_source' column and extract 'tweet_id'\n",
    "def extract_tweet_id(source_str):\n",
    "    try:\n",
    "        # Convert string to dictionary\n",
    "        source_dict = ast.literal_eval(source_str)\n",
    "        # Navigate to 'tweet_id' within the parsed dictionary\n",
    "        return source_dict.get('tweet', {}).get('tweet_id', None)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return None\n",
    "\n",
    "\n",
    "predicted = pd.read_csv('predictions.csv')\n",
    "print(predicted.columns)\n",
    "# Apply the function to extract 'tweet_id' and create a new 'id' column\n",
    "predicted['id'] = predicted['_source'].apply(lambda x:  extract_tweet_id(x))\n",
    "\n",
    "new_predicted = predicted[['id', 'predicted_emotion']]\n",
    "\n",
    "new_predicted.rename(columns = {\"predicted_emotion\" : \"emotion\"}, inplace = True)\n",
    "\n",
    "print(new_predicted.head(5))\n",
    "\n",
    "new_predicted.to_csv('prediction1127.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
